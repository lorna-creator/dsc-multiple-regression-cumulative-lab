{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Logistic Regression - Cumulative Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this cumulative lab, you will walk through a complete machine learning workflow with logistic regression, including data preparation, modeling (including hyperparameter tuning), and final model evaluation.\n",
    "\n",
=======
    "# Multiple Linear Regression - Cumulative Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this cumulative lab you'll perform an end-to-end analysis of a dataset using multiple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
<<<<<<< HEAD
    "* Practice identifying and applying appropriate preprocessing steps\n",
    "* Perform an iterative modeling process, starting from a baseline model\n",
    "* Practice model validation\n",
    "* Practice choosing a final logistic regression model and evaluating its performance"
=======
    "* Prepare data for regression analysis using pandas\n",
    "* Build multiple linear regression models using StatsModels\n",
    "* Measure regression model performance\n",
    "* Interpret multiple linear regression coefficients"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Your Task: Complete an End-to-End ML Process with Logistic Regression on the Forest Cover Dataset\n",
    "\n",
    "![forest road](https://curriculum-content.s3.amazonaws.com/data-science/images/forest_road.jpg)\n",
    "\n",
    "<span>Photo by <a href=\"https://unsplash.com/@von_co?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Ivana Cajina</a> on <a href=\"https://unsplash.com/s/photos/forest-satellite?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></span>"
=======
    "## Your Task: Develop a Model of Diamond Prices\n",
    "\n",
    "![tweezers holding a diamond](https://curriculum-content.s3.amazonaws.com/data-science/images/diamond.jpg)\n",
    "\n",
    "Photo by <a href=\"https://unsplash.com/@tahliaclaire?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Tahlia Doyle</a> on <a href=\"https://unsplash.com/s/photos/diamonds?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Business and Data Understanding\n",
    "\n",
    "Here we will be using an adapted version of the forest cover dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/covertype). Each record represents a 30 x 30 meter cell of land within Roosevelt National Forest in northern Colorado, which has been labeled as `Cover_Type` 1 for \"Cottonwood/Willow\" and `Cover_Type` 0 for \"Ponderosa Pine\". (The original dataset contained 7 cover types but we have simplified it.)\n",
    "\n",
    "The task is to predict the `Cover_Type` based on the available cartographic variables:"
=======
    "### Business Understanding\n",
    "\n",
    "You've been asked to perform an analysis to see how various factors impact the price of diamonds. There are various [guides online](https://www.diamonds.pro/education/diamond-prices/) that claim to tell consumers how to avoid getting \"ripped off\", but you've been asked to dig into the data to see whether these claims ring true.\n",
    "\n",
    "### Data Understanding\n",
    "\n",
    "We have downloaded a diamonds dataset from [Kaggle](https://www.kaggle.com/datasets/shivam2503/diamonds), which came with this description:\n",
    "\n",
    "* **price** price in US dollars (\\$326--\\$18,823)\n",
    "* **carat** weight of the diamond (0.2--5.01)\n",
    "* **cut** quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "* **color** diamond colour, from J (worst) to D (best)\n",
    "* **clarity** a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "* **x** length in mm (0--10.74)\n",
    "* **y** width in mm (0--58.9)\n",
    "* **z** depth in mm (0--31.8)\n",
    "* **depth** total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
    "* **table** width of top of diamond relative to widest point (43--95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "#### 1. Load the Data Using Pandas\n",
    "\n",
    "Practice once again with loading CSV data into a `pandas` dataframe.\n",
    "\n",
    "#### 2. Build a Baseline Simple Linear Regression Model\n",
    "\n",
    "Identify the feature that is most correlated with `price` and build a StatsModels linear regression model using just that feature.\n",
    "\n",
    "#### 3. Evaluate and Interpret Baseline Model Results\n",
    "\n",
    "Explain the overall performance as well as parameter coefficients for the baseline simple linear regression model.\n",
    "\n",
    "#### 4. Prepare a Categorical Feature for Multiple Regression Modeling\n",
    "\n",
    "Identify a promising categorical feature and use `pd.get_dummies()` to prepare it for modeling.\n",
    "\n",
    "#### 5. Build a Multiple Linear Regression Model\n",
    "\n",
    "Using the data from Step 4, create a second StatsModels linear regression model using one numeric feature and one one-hot encoded categorical feature.\n",
    "\n",
    "#### 6. Evaluate and Interpret Multiple Linear Regression Model Results\n",
    "\n",
    "Explain the performance of the new model in comparison with the baseline, and interpret the new parameter coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data Using Pandas\n",
    "\n",
    "Import `pandas` (with the standard alias `pd`), and load the data from the file `diamonds.csv` into a DataFrame called `diamonds`.\n",
    "\n",
    "Be sure to specify `index_col=0` to avoid creating an \"Unnamed: 0\" column."
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 21,
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
<<<<<<< HEAD
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type_31</th>\n",
       "      <th>Soil_Type_32</th>\n",
       "      <th>Soil_Type_33</th>\n",
       "      <th>Soil_Type_34</th>\n",
       "      <th>Soil_Type_35</th>\n",
       "      <th>Soil_Type_36</th>\n",
       "      <th>Soil_Type_37</th>\n",
       "      <th>Soil_Type_38</th>\n",
       "      <th>Soil_Type_39</th>\n",
       "      <th>Cover_Type</th>\n",
=======
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>0</th>\n",
       "      <td>2553</td>\n",
       "      <td>235</td>\n",
       "      <td>17</td>\n",
       "      <td>351</td>\n",
       "      <td>95</td>\n",
       "      <td>780</td>\n",
       "      <td>188</td>\n",
       "      <td>253</td>\n",
       "      <td>199</td>\n",
       "      <td>1410</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>344</td>\n",
       "      <td>17</td>\n",
       "      <td>313</td>\n",
       "      <td>29</td>\n",
       "      <td>404</td>\n",
       "      <td>183</td>\n",
       "      <td>211</td>\n",
       "      <td>164</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>391</td>\n",
       "      <td>42</td>\n",
       "      <td>509</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>134</td>\n",
       "      <td>421</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2038</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>408</td>\n",
       "      <td>71</td>\n",
       "      <td>474</td>\n",
       "      <td>226</td>\n",
       "      <td>200</td>\n",
       "      <td>102</td>\n",
       "      <td>283</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>341</td>\n",
       "      <td>27</td>\n",
       "      <td>351</td>\n",
       "      <td>34</td>\n",
       "      <td>390</td>\n",
       "      <td>152</td>\n",
       "      <td>188</td>\n",
       "      <td>168</td>\n",
       "      <td>190</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
=======
       "      <th>1</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
<<<<<<< HEAD
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38496</th>\n",
       "      <td>2396</td>\n",
       "      <td>153</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>108</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>118</td>\n",
       "      <td>837</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38497</th>\n",
       "      <td>2391</td>\n",
       "      <td>152</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>119</td>\n",
       "      <td>845</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38498</th>\n",
       "      <td>2386</td>\n",
       "      <td>159</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>236</td>\n",
       "      <td>241</td>\n",
       "      <td>130</td>\n",
       "      <td>854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38499</th>\n",
       "      <td>2384</td>\n",
       "      <td>170</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>230</td>\n",
       "      <td>245</td>\n",
       "      <td>143</td>\n",
       "      <td>864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38500</th>\n",
       "      <td>2383</td>\n",
       "      <td>165</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>231</td>\n",
       "      <td>244</td>\n",
       "      <td>141</td>\n",
       "      <td>875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38501 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0           2553     235     17                               351   \n",
       "1           2011     344     17                               313   \n",
       "2           2022      24     13                               391   \n",
       "3           2038      50     17                               408   \n",
       "4           2018     341     27                               351   \n",
       "...          ...     ...    ...                               ...   \n",
       "38496       2396     153     20                                85   \n",
       "38497       2391     152     19                                67   \n",
       "38498       2386     159     17                                60   \n",
       "38499       2384     170     15                                60   \n",
       "38500       2383     165     13                                60   \n",
       "\n",
       "       Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                                  95                              780   \n",
       "1                                  29                              404   \n",
       "2                                  42                              509   \n",
       "3                                  71                              474   \n",
       "4                                  34                              390   \n",
       "...                               ...                              ...   \n",
       "38496                              17                              108   \n",
       "38497                              12                               95   \n",
       "38498                               7                               90   \n",
       "38499                               5                               90   \n",
       "38500                               4                               67   \n",
       "\n",
       "       Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0                188             253            199   \n",
       "1                183             211            164   \n",
       "2                212             212            134   \n",
       "3                226             200            102   \n",
       "4                152             188            168   \n",
       "...              ...             ...            ...   \n",
       "38496            240             237            118   \n",
       "38497            240             237            119   \n",
       "38498            236             241            130   \n",
       "38499            230             245            143   \n",
       "38500            231             244            141   \n",
       "\n",
       "       Horizontal_Distance_To_Fire_Points  ...  Soil_Type_31  Soil_Type_32  \\\n",
       "0                                    1410  ...             0             0   \n",
       "1                                     300  ...             0             0   \n",
       "2                                     421  ...             0             0   \n",
       "3                                     283  ...             0             0   \n",
       "4                                     190  ...             0             0   \n",
       "...                                   ...  ...           ...           ...   \n",
       "38496                                 837  ...             0             0   \n",
       "38497                                 845  ...             0             0   \n",
       "38498                                 854  ...             0             0   \n",
       "38499                                 864  ...             0             0   \n",
       "38500                                 875  ...             0             0   \n",
       "\n",
       "       Soil_Type_33  Soil_Type_34  Soil_Type_35  Soil_Type_36  Soil_Type_37  \\\n",
       "0                 0             0             0             0             0   \n",
       "1                 0             0             0             0             0   \n",
       "2                 0             0             0             0             0   \n",
       "3                 0             0             0             0             0   \n",
       "4                 0             0             0             0             0   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "38496             0             0             0             0             0   \n",
       "38497             0             0             0             0             0   \n",
       "38498             0             0             0             0             0   \n",
       "38499             0             0             0             0             0   \n",
       "38500             0             0             0             0             0   \n",
       "\n",
       "       Soil_Type_38  Soil_Type_39  Cover_Type  \n",
       "0                 0             0           0  \n",
       "1                 0             0           0  \n",
       "2                 0             0           0  \n",
       "3                 0             0           0  \n",
       "4                 0             0           0  \n",
       "...             ...           ...         ...  \n",
       "38496             0             0           0  \n",
       "38497             0             0           0  \n",
       "38498             0             0           0  \n",
       "38499             0             0           0  \n",
       "38500             0             0           0  \n",
       "\n",
       "[38501 rows x 53 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/forest_cover.csv')  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have over 38,000 rows, each with 52 feature columns and 1 target column:\n",
    "\n",
    "* `Elevation`: Elevation in meters\n",
    "* `Aspect`: Aspect in degrees azimuth\n",
    "* `Slope`: Slope in degrees\n",
    "* `Horizontal_Distance_To_Hydrology`: Horizontal dist to nearest surface water features in meters\n",
    "* `Vertical_Distance_To_Hydrology`: Vertical dist to nearest surface water features in meters\n",
    "* `Horizontal_Distance_To_Roadways`: Horizontal dist to nearest roadway in meters\n",
    "* `Hillshade_9am`: Hillshade index at 9am, summer solstice\n",
    "* `Hillshade_Noon`: Hillshade index at noon, summer solstice\n",
    "* `Hillshade_3pm`: Hillshade index at 3pm, summer solstice\n",
    "* `Horizontal_Distance_To_Fire_Points`: Horizontal dist to nearest wildfire ignition points, meters\n",
    "* `Wilderness_Area_x`: Wilderness area designation (3 columns)\n",
    "* `Soil_Type_x`: Soil Type designation (39 columns)\n",
    "* `Cover_Type`: 1 for cottonwood/willow, 0 for ponderosa pine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also an imbalanced dataset, since cottonwood/willow trees are relatively rare in this forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Counts\n",
      "0    35754\n",
      "1     2747\n",
      "Name: Cover_Type, dtype: int64\n",
      "\n",
      "Percentages\n",
      "0    0.928651\n",
      "1    0.071349\n",
      "Name: Cover_Type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "print(\"Raw Counts\")\n",
    "print(df[\"Cover_Type\"].value_counts())\n",
    "print()\n",
    "print(\"Percentages\")\n",
    "print(df[\"Cover_Type\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a model that *always* said that the cover type was ponderosa pine (class 0), what accuracy score would we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n92% percent of this data is correct.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate text\n",
    "\"\"\"\n",
    "92% percent of this data is correct.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to take this into account when working through this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "#### 1. Perform a Train-Test Split\n",
    "\n",
    "For a complete end-to-end ML process, we need to create a holdout set that we will use at the very end to evaluate our final model's performance.\n",
    "\n",
    "#### 2. Build and Evaluate a Baseline Model\n",
    "\n",
    "Without performing any preprocessing or hyperparameter tuning, build and evaluate a vanilla logistic regression model using log loss and `cross_val_score`.\n",
    "\n",
    "#### 3. Write a Custom Cross Validation Function\n",
    "\n",
    "Because we are using preprocessing techniques that differ for train and validation data, we will need a custom function rather than simply preprocessing the entire `X_train` and using `cross_val_score` from scikit-learn.\n",
    "\n",
    "#### 4. Build and Evaluate Additional Logistic Regression Models\n",
    "\n",
    "Using the function created in the previous step, build multiple logistic regression models with different hyperparameters in order to minimize log loss.\n",
    "\n",
    "#### 5. Choose and Evaluate a Final Model\n",
    "\n",
    "Preprocess the full training set and test set appropriately, then evaluate the final model with various classification metrics in addition to log loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform a Train-Test Split\n",
    "\n",
    "This process should be fairly familiar by now. In the cell below, use the variable `df` (that has already been created) in order to create `X` and `y`, then training and test sets using `train_test_split` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n",
    "\n",
    "We'll use a random state of 42 and `stratify=y` (to ensure an even balance of tree types) in the train-test split. Recall that the target is `Cover_Type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split df into X and y\n",
    "X = df.drop(\"Cover_Type\", axis=1)\n",
    "y = df[\"Cover_Type\"]\n",
    "\n",
    "# Perform train-test split with random_state=42 and stratify=y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you have the correct data shape before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# X and y training data should have the same number of rows\n",
    "assert X_train.shape[0] == y_train.shape[0] and X_train.shape[0] == 28875\n",
    "\n",
    "# X and y testing data should have the same number of rows\n",
    "assert X_test.shape[0] == y_test.shape[0] and X_test.shape[0] == 9626\n",
    "\n",
    "# Both X should have 52 columns\n",
    "assert X_train.shape[1] == X_test.shape[1] and X_train.shape[1] == 52\n",
    "\n",
    "# Both y should have 1 column\n",
    "assert len(y_train.shape) == len(y_test.shape) and len(y_train.shape) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we should have roughly equal percentages of cottonwood/willow trees for train vs. test targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train percent cottonwood/willow: 0.07134199134199135\n",
      "Test percent cottonwood/willow:  0.0713692083939331\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "print(\"Train percent cottonwood/willow:\", y_train.value_counts(normalize=True)[1])\n",
    "print(\"Test percent cottonwood/willow: \", y_test.value_counts(normalize=True)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build and Evaluate a Baseline Model\n",
    "\n",
    "Using scikit-learn's `LogisticRegression` model, instantiate a classifier with `random_state=42`. Then use `cross_val_score` with `scoring=\"neg_log_loss\"` to find the average cross-validated log loss for this model on `X_train` and `y_train`.\n",
    "\n",
    "* [`LogisticRegression` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "* [`cross_val_score` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "\n",
    "(Similar to RMSE, the internal implementation of `cross_val_score` requires that we use \"negative log loss\" instead of just log loss. The code provided negates the result for you.)\n",
    "\n",
    "**The code below should produce a warning** but not an error. Because we have not scaled the data, we expect to get a `ConvergenceWarning` five times (once for each fold of cross validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1723335580967297"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "# Import relevant class and function\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a LogisticRegression with random_state=42\n",
    "baseline_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Use cross_val_score with scoring=\"neg_log_loss\" to evaluate the model\n",
    "# on X_train and y_train\n",
    "baseline_neg_log_loss_cv = cross_val_score(baseline_model, X_train, y_train, scoring=\"neg_log_loss\")\n",
    "\n",
    "baseline_log_loss = -(baseline_neg_log_loss_cv.mean())\n",
    "baseline_log_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we are getting the `ConvergenceWarning`s we expected, and log loss of around 0.172 with our baseline model.\n",
    "\n",
    "Is that a \"good\" log loss? That's hard to say — log loss is not particularly interpretable. \n",
    "\n",
    "If we had a model that just chose 0 (the majority class) every time, this is the log loss we would get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4640650865286937"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "log_loss(y_train, np.zeros(len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is a metric where lower is better, so our baseline model is clearly an improvement over just guessing the majority class every time.\n",
    "\n",
    "Even though it is difficult to interpret, the 0.172 value will be a useful baseline as we continue modeling, to see if we are actually making improvements or just getting slightly better performance by chance.\n",
    "\n",
    "We will also use other metrics at the last step in order to describe the final model's performance in a more user-friendly way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write a Custom Cross Validation Function\n",
    "\n",
    "### Conceptual Considerations\n",
    "\n",
    "First, consider: which preprocessing steps should be taken with this dataset? Recall that our data is imbalanced, and that it caused a `ConvergenceWarning` for our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe should use some kind of resampling technique to address the large class of imbalance. \"Smote\"\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate text\n",
    "\"\"\"\n",
    "We should use some kind of resampling technique to address the large class of imbalance. \"Smote\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you likely noted above, we should use some kind of resampling technique to address the large class imbalance. Let's use `SMOTE` (synthetic minority oversampling, [documentation here](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)), which creates synthetic examples of the minority class to help train the model.\n",
    "\n",
    "Does SMOTE work just like a typical scikit-learn transformer, where you fit the transformer on the training data then transform both the training and the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n___\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate text\n",
    "\"\"\"\n",
    "___\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNo, SMOTE does not work like that. We never want to oversample the\\nminority class in the test data, because then we are generating\\nmetrics based on synthetic data and not actual data.\\n\\nInstead, we only want to fit and transform the training data, and\\nleave the testing data alone.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION\n",
    "\"\"\"\n",
    "No, SMOTE does not work like that. We never want to oversample the\n",
    "minority class in the test data, because then we are generating\n",
    "metrics based on synthetic data and not actual data.\n",
    "\n",
    "Instead, we only want to fit and transform the training data, and\n",
    "leave the testing data alone.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you also likely noted above, we should use some transformer to normalize the data. Let's use a `StandardScaler` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)).\n",
    "\n",
    "Does `StandardScaler` work just like a typical scikit-learn transformer, where you fit the transformer on the training data then transform both the training and the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYes, StandardScaler is a scikit-learn transformer so it does work\\nthis way. We transform both the train and test data, using the fit\\nfrom the training data only.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate text\n",
    "\"\"\"\n",
    "Yes, StandardScaler is a scikit-learn transformer so it does work\n",
    "this way. We transform both the train and test data, using the fit\n",
    "from the training data only.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(At this point it's a good idea to double-check your answers against the `solution` branch to make sure you understand the setup.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `StratifiedKFold`\n",
    "\n",
    "As you can see from the `cross_val_score` documentation linked above, \"under the hood\" it is using `StratifiedKFold` for classification tasks.\n",
    "\n",
    "Essentially `StratifiedKFold` is just providing the information you need to make 5 separate train-test splits inside of `X_train`. Then there is other logic within `cross_val_score` to fit and evaluate the provided model.\n",
    "\n",
    "So, if our original code looked like this:\n",
    "\n",
    "```python\n",
    "baseline_model = LogisticRegression(random_state=42)\n",
    "baseline_neg_log_loss_cv = cross_val_score(baseline_model, X_train, y_train, scoring=\"neg_log_loss\")\n",
    "baseline_log_loss = -(baseline_neg_log_loss_cv.mean())\n",
    "baseline_log_loss\n",
    "```\n",
    "\n",
    "The equivalent of the above code using `StratifiedKFold` would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1723335580967297"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Negative log loss doesn't exist as something we can import,\n",
    "# but we can create it\n",
    "neg_log_loss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "# Instantiate the model (same as previous example)\n",
    "baseline_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create a list to hold the score from each fold\n",
    "kfold_scores = np.ndarray(5)\n",
    "\n",
    "# Instantiate a splitter object and loop over its result\n",
    "kfold = StratifiedKFold()\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # Extract train and validation subsets using the provided indices\n",
    "    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Clone the provided model and fit it on the train subset\n",
    "    temp_model = clone(baseline_model)\n",
    "    temp_model.fit(X_t, y_t)\n",
    "    \n",
    "    # Evaluate the provided model on the validation subset\n",
    "    neg_log_loss_score = neg_log_loss(temp_model, X_val, y_val)\n",
    "    kfold_scores[fold] = neg_log_loss_score\n",
    "    \n",
    "-(kfold_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this produced the same result as our original cross validation (including the `ConvergenceWarning`s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.17160037 -0.17457737 -0.16308199 -0.17948594 -0.17292212]\n",
      "[-0.17160037 -0.17457737 -0.16308199 -0.17948594 -0.17292212]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "print(baseline_neg_log_loss_cv)\n",
    "print(kfold_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is the point of doing it this way, instead of the much-shorter `cross_val_score` approach?\n",
    "\n",
    "**Using `StratifiedKFold` \"by hand\" allows us to customize what happens inside of that loop.**\n",
    "\n",
    "Therefore we can apply these preprocessing techniques appropriately:\n",
    "\n",
    "1. Fit a `StandardScaler` object on the training subset (not the full training data) and transform both the train and test subsets\n",
    "2. Fit a `SMOTE` object and transform only the training subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a Custom Cross Validation Function with `StratifiedKFold`\n",
    "\n",
    "In the cell below, we have set up a function `custom_cross_val_score` that has an interface that resembles the `cross_val_score` function from scikit-learn.\n",
    "\n",
    "Most of it is set up for you already, all you need to do is add the `SMOTE` and `StandardScaler` steps described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1323589955073186"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import relevant sklearn and imblearn classes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def custom_cross_val_score(estimator, X, y):\n",
    "    # Create a list to hold the scores from each fold\n",
    "    kfold_train_scores = np.ndarray(5)\n",
    "    kfold_val_scores = np.ndarray(5)\n",
    "\n",
    "    # Instantiate a splitter object and loop over its result\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X, y)):\n",
    "        # Extract train and validation subsets using the provided indices\n",
    "        X_t, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_t, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        # Instantiate StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        # Fit and transform X_t\n",
    "        X_t_scaled = scaler.fit_transform(X_t)\n",
    "        # Transform X_val\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        # Instantiate SMOTE with random_state=42 and sampling_strategy=0.28\n",
    "        sm = SMOTE(random_state=42, sampling_strategy=0.28)\n",
    "        # Fit and transform X_t_scaled and y_t using sm\n",
    "        X_t_oversampled, y_t_oversampled = sm.fit_resample(X_t_scaled,y_t)\n",
    "        \n",
    "        # Clone the provided model and fit it on the train subset\n",
    "        temp_model = clone(estimator)\n",
    "        temp_model.fit(X_t_oversampled, y_t_oversampled)\n",
    "        \n",
    "        # Evaluate the provided model on the train and validation subsets\n",
    "        neg_log_loss_score_train = neg_log_loss(temp_model, X_t_oversampled, y_t_oversampled)\n",
    "        neg_log_loss_score_val = neg_log_loss(temp_model, X_val_scaled, y_val)\n",
    "        kfold_train_scores[fold] = neg_log_loss_score_train\n",
    "        kfold_val_scores[fold] = neg_log_loss_score_val\n",
    "        \n",
    "    return kfold_train_scores, kfold_val_scores\n",
    "        \n",
    "model_with_preprocessing = LogisticRegression(random_state=42, class_weight={1: 0.28})\n",
    "preprocessed_train_scores, preprocessed_neg_log_loss_cv = custom_cross_val_score(model_with_preprocessing, X_train, y_train)\n",
    "- (preprocessed_neg_log_loss_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output you get should be about 0.132, and there should no longer be a `ConvergenceWarning`.\n",
    "\n",
    "If you're not getting the right output, double check that you are applying the correct transformations to the correct variables:\n",
    "\n",
    "1. `X_t` should be scaled to create `X_t_scaled`, then `X_t_scaled` should be resampled to create `X_t_oversampled`, then `X_t_oversampled` should be used to fit the model\n",
    "2. `X_val` should be scaled to create `X_val_scaled`, then `X_val_scaled` should be used to evaluate `neg_log_loss`\n",
    "3. `y_t` should be resampled to create `y_t_oversampled`, then `y_t_oversampled` should be used to fit the model\n",
    "4. `y_val` should not be transformed in any way. It should just be used to evaluate `neg_log_loss`\n",
    "\n",
    "Another thing to check is that you used `sampling_strategy=0.28` when you instantiated the `SMOTE` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are getting the right output, great!  Let's compare that to our baseline log loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1723335580967297\n",
      "0.1323589955073186\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "print(-baseline_neg_log_loss_cv.mean())\n",
    "print(-preprocessed_neg_log_loss_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our preprocessing with `StandardScaler` and `SMOTE` has provided some improvement over the baseline! Let's move on to Step 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build and Evaluate Additional Logistic Regression Models\n",
    "\n",
    "Now that we have applied appropriate preprocessing steps to our data in our custom cross validation function, we can reuse that function to test multiple different `LogisticRegression` models.\n",
    "\n",
    "For each model iteration, make sure you specify `class_weight={1: 0.28}`, because this aligns with the weighting created by our `SMOTE` process.\n",
    "\n",
    "### Where to Next?\n",
    "\n",
    "One of the first questions to ask when you start iterating on any model is: ***are we overfitting***? Many of the models you will learn during this course will have built-in functionality to reduce overfitting.\n",
    "\n",
    "To determine whether we are overfitting, let's examine the training scores vs. the validation scores from our existing modeling process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:      [0.29227141 0.28801243 0.29282026 0.28652204 0.28910185]\n",
      "Validation: [0.13067576 0.13636961 0.12628191 0.13715658 0.13131112]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "print(\"Train:     \", -preprocessed_train_scores)\n",
    "print(\"Validation:\", -preprocessed_neg_log_loss_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that these are loss metrics, meaning lower is better. Are we overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNo because we are kind of challenging to compare these numbers directly, so it does not appear that we are overfitting. If\\nwe were overfitting would mean getting better scores on the training data than the validation data\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate text\n",
    "\"\"\"\n",
    "No because we are kind of challenging to compare these numbers directly, so it does not appear that we are overfitting. If\n",
    "we were overfitting would mean getting better scores on the training data than the validation data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's actually possible that we are underfitting due to too high of regularization. Remember that `LogisticRegression` from scikit-learn has regularization by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': {1: 0.28},\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': 42,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
=======
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53936</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>60.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53937</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.66</td>\n",
       "      <td>5.68</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.86</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53940</th>\n",
       "      <td>0.75</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53940 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table  price     x     y     z\n",
       "1       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "2       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "3       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "4       0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "5       0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
       "...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...\n",
       "53936   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75  5.76  3.50\n",
       "53937   0.72       Good     D     SI1   63.1   55.0   2757  5.69  5.75  3.61\n",
       "53938   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66  5.68  3.56\n",
       "53939   0.86    Premium     H     SI2   61.0   58.0   2757  6.15  6.12  3.74\n",
       "53940   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83  5.87  3.64\n",
       "\n",
       "[53940 rows x 10 columns]"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Run this cell without changes\n",
    "model_with_preprocessing.get_params()"
=======
    "# Your code here\n",
    "import pandas as pd\n",
    "diamonds = pd.read_csv('diamonds.csv', index_col=0)\n",
    "diamonds\n",
    "\n"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "That first key-value pair, `'C': 1.0`, specifies the regularization strength. As is noted in the [scikit-learn `LogisticRegression` docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), `C` is:\n",
    "\n",
    "> Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "\n",
    "In general if you are increasing `C` you want to increase it by orders of magnitude. I.e. not increasing it to 1.1, but rather increasing it to 1e3, 1e5, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Regularization\n",
    "\n",
    "In the cell below, instantiate a `LogisticRegression` model with lower regularization (i.e. higher `C`), along with `random_state=42` and `class_weight={1: 0.28}`. Call this model `model_less_regularization`."
=======
    "The following code checks that you loaded the data correctly:"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Replace None with appropriate code\n",
    "\n",
    "\n",
    "model_less_regularization = LogisticRegression(\n",
    "    random_state=42,\n",
    "    class_weight={1: 0.28},\n",
    "    C=1e5,\n",
    "    solver=\"saga\",\n",
    "    penalty=\"elasticnet\",\n",
    "    l1_ratio=0.5,\n",
    "    max_iter=2000\n",
    ")"
=======
    "# Run this cell without changes\n",
    "\n",
    "# diamonds should be a dataframe\n",
    "assert type(diamonds) == pd.DataFrame\n",
    "\n",
    "# Check that there are the correct number of rows\n",
    "assert diamonds.shape[0] == 53940\n",
    "\n",
    "# Check that there are the correct number of columns\n",
    "# (if this crashes, make sure you specified `index_col=0`)\n",
    "assert diamonds.shape[1] == 10"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "This code cell double-checks that the model was created correctly:"
=======
    "Inspect the distributions of the numeric features:"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Check variable type\n",
    "assert type(model_less_regularization) == LogisticRegression\n",
    "\n",
    "# Check params\n",
    "assert model_less_regularization.get_params()[\"random_state\"] == 42\n",
    "assert model_less_regularization.get_params()[\"class_weight\"] == {1: 0.28}\n",
    "assert model_less_regularization.get_params()[\"C\"] != 1.0"
=======
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.797940</td>\n",
       "      <td>61.749405</td>\n",
       "      <td>57.457184</td>\n",
       "      <td>3932.799722</td>\n",
       "      <td>5.731157</td>\n",
       "      <td>5.734526</td>\n",
       "      <td>3.538734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.474011</td>\n",
       "      <td>1.432621</td>\n",
       "      <td>2.234491</td>\n",
       "      <td>3989.439738</td>\n",
       "      <td>1.121761</td>\n",
       "      <td>1.142135</td>\n",
       "      <td>0.705699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2401.000000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>5324.250000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.010000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>18823.000000</td>\n",
       "      <td>10.740000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>31.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table         price             x  \\\n",
       "count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \n",
       "mean       0.797940     61.749405     57.457184   3932.799722      5.731157   \n",
       "std        0.474011      1.432621      2.234491   3989.439738      1.121761   \n",
       "min        0.200000     43.000000     43.000000    326.000000      0.000000   \n",
       "25%        0.400000     61.000000     56.000000    950.000000      4.710000   \n",
       "50%        0.700000     61.800000     57.000000   2401.000000      5.700000   \n",
       "75%        1.040000     62.500000     59.000000   5324.250000      6.540000   \n",
       "max        5.010000     79.000000     95.000000  18823.000000     10.740000   \n",
       "\n",
       "                  y             z  \n",
       "count  53940.000000  53940.000000  \n",
       "mean       5.734526      3.538734  \n",
       "std        1.142135      0.705699  \n",
       "min        0.000000      0.000000  \n",
       "25%        4.720000      2.910000  \n",
       "50%        5.710000      3.530000  \n",
       "75%        6.540000      4.040000  \n",
       "max       58.900000     31.800000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "diamonds.describe()"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Now, evaluate that model using `custom_cross_val_score`. Recall that `custom_cross_val_score` takes 3 arguments: `estimator`, `X`, and `y`. In this case, `estimator` should be `model_less_regularization`, `X` should be `X_train`, and `y` should be `y_train`."
=======
    "And inspect the value counts for the categorical features:"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Previous Model\n",
      "Train average:      0.28974560104091546\n",
      "Validation average: 0.1323589955073186\n",
      "Current Model\n",
      "Train average:      0.28973220231874425\n",
      "Validation average: 0.13241907028228445\n"
=======
      "Ideal        21551\n",
      "Premium      13791\n",
      "Very Good    12082\n",
      "Good          4906\n",
      "Fair          1610\n",
      "Name: cut, dtype: int64 \n",
      "\n",
      "G    11292\n",
      "E     9797\n",
      "F     9542\n",
      "H     8304\n",
      "D     6775\n",
      "I     5422\n",
      "J     2808\n",
      "Name: color, dtype: int64 \n",
      "\n",
      "SI1     13065\n",
      "VS2     12258\n",
      "SI2      9194\n",
      "VS1      8171\n",
      "VVS2     5066\n",
      "VVS1     3655\n",
      "IF       1790\n",
      "I1        741\n",
      "Name: clarity, dtype: int64 \n",
      "\n"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Replace None with appropriate code\n",
    "less_regularization_train_scores, less_regularization_val_scores = custom_cross_val_score(\n",
    "    model_less_regularization,\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Previous Model\")\n",
    "print(\"Train average:     \", -preprocessed_train_scores.mean())\n",
    "print(\"Validation average:\", -preprocessed_neg_log_loss_cv.mean())\n",
    "print(\"Current Model\")\n",
    "print(\"Train average:     \", -less_regularization_train_scores.mean())\n",
    "print(\"Validation average:\", -less_regularization_val_scores.mean())"
=======
    "# Run this cell without changes\n",
    "categoricals = diamonds.select_dtypes(\"object\")\n",
    "\n",
    "for col in categoricals:\n",
    "    print(diamonds[col].value_counts(), \"\\n\")"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Your answers will vary somewhat depending on the value of `C` that you chose, but in general you should see a slight improvement, from something like 0.132358 validation average to 0.132344 (improvement of .000014). Not a massive difference but it is an improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Solver\n",
    "\n",
    "Right now we are using the default solver and type of regularization penalty:"
=======
    "## 2. Build a Baseline Simple Linear Regression Model\n",
    "\n",
    "### Identifying a Highly Correlated Predictor\n",
    "\n",
    "The target variable is `price`. Look at the correlation coefficients for all of the predictor variables to find the one with the highest correlation with `price`."
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solver: saga\n",
      "penalty: elasticnet\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "print(\"solver:\", model_less_regularization.get_params()[\"solver\"])\n",
    "print(\"penalty:\", model_less_regularization.get_params()[\"penalty\"])"
=======
     "data": {
      "text/plain": [
       "carat    0.921591\n",
       "depth   -0.010647\n",
       "table    0.127134\n",
       "x        0.884435\n",
       "y        0.865421\n",
       "z        0.861249\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here - look at correlations\n",
    "correlations = diamonds.select_dtypes(include=['float64', 'int64'])\n",
    "correlations.corr()[\"price\"].drop('price')\n"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "What if we want to try a different kind of regularization penalty?\n",
    "\n",
    "From the docs:\n",
    "\n",
    "> * ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n",
    "> * ‘liblinear’ and ‘saga’ also handle L1 penalty\n",
    "> * ‘saga’ also supports ‘elasticnet’ penalty\n",
    "\n",
    "In other words, the only models that support L1 or elastic net penalties are `liblinear` and `saga`. `liblinear` is going to be quite slow with the size of our dataset, so let's use `saga`.\n",
    "\n",
    "In the cell below, create a model that uses `solver=\"saga\"` and `penalty=\"elasticnet\"`. Then use the `l1_ratio` argument to specify the mixing of L1 and L2 regularization. You want a value greater than zero (zero would just mean using L2 regularization) and less than one (one would mean using just L1 regularization).\n",
    "\n",
    "Remember to also specify `random_state=42`, `class_weight={1: 0.28}`, and `C` equals the value you previously used."
=======
    "Identify the name of the predictor column with the strongest correlation below."
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Model (Less Regularization)\n",
      "Train average:      0.28973220231874425\n",
      "Validation average: 0.13241907028228445\n",
      "Current Model\n",
      "Train average:      0.29297684099860494\n",
      "Validation average: 0.13604493761082836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gichuhi\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
=======
     "data": {
      "text/plain": [
       "'carat'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
<<<<<<< HEAD
    "model_alternative_solver = LogisticRegression(\n",
    "    random_state=42,\n",
    "    class_weight={1: 0.28},\n",
    "    C=1e5,\n",
    "    solver=\"saga\",\n",
    "    penalty=\"elasticnet\",\n",
    "    l1_ratio=0.5\n",
    ")\n",
    "\n",
    "alternative_solver_train_scores, alternative_solver_val_scores = custom_cross_val_score(\n",
    "    model_alternative_solver,\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "print(\"Previous Model (Less Regularization)\")\n",
    "print(\"Train average:     \", -less_regularization_train_scores.mean())\n",
    "print(\"Validation average:\", -less_regularization_val_scores.mean())\n",
    "print(\"Current Model\")\n",
    "print(\"Train average:     \", -alternative_solver_train_scores.mean())\n",
    "print(\"Validation average:\", -alternative_solver_val_scores.mean())"
=======
    "most_correlated = 'carat'\n",
    "most_correlated"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Most likely you started getting `ConvergenceWarning`s again, even though we are scaling the data inside of `custom_cross_val_score`. When you get a convergence warning in a case like this, you want to modify the `tol` and/or `max_iter` parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Gradient Descent Parameters\n",
    "\n",
    "If you are getting good results (good metrics) but are still getting a `ConvergenceWarning`, consider increasing the tolerance (`tol` argument). The tolerance specifies how close to zero the gradient must be in order to stop taking additional steps. It's possible that your model is finding a gradient that is close enough to zero, but slightly above the default tolerance, if everything otherwise looks good.\n",
    "\n",
    "In this case, we are getting slightly worse metrics on both the train and the validation data (compared to a different solver strategy), so increasing the number of iterations (`max_iter`) seems like a better strategy. Essentially this is allowing the gradient descent algorithm to take more steps as it tries to find an optimal solution.\n",
    "\n",
    "In the cell below, create a model called `model_more_iterations` that has the same hyperparameters as `model_alternative_solver`, with the addition of an increased `max_iter`. You'll need to increase `max_iter` significantly to a number in the thousands.\n",
    "\n",
    "**Note:** As you increase `max_iter`, it is normal for the execution time of fitting the model to increase. The following cell may take up to several minutes to execute. Try to be patient with this exercise! If this step times out, you can just read on ahead."
=======
    "The following code checks that you specified a column correctly:"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Best Model (Less Regularization)\n",
      "Train average:      0.28973220231874425\n",
      "Validation average: 0.13241907028228445\n",
      "Previous Model with This Solver\n",
      "Train average:      0.29297684099860494\n",
      "Validation average: 0.13604493761082836\n",
      "Current Model\n",
      "Train average:      0.28973220231874425\n",
      "Validation average: 0.13241907028228445\n"
     ]
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "model_more_iterations =  LogisticRegression(\n",
    "    random_state=42,\n",
    "    class_weight={1: 0.28},\n",
    "    C=1e5,\n",
    "    solver=\"saga\",\n",
    "    penalty=\"elasticnet\",\n",
    "    l1_ratio=0.5,\n",
    "    max_iter=2000\n",
    ")\n",
    "\n",
    "more_iterations_train_scores, more_iterations_val_scores = custom_cross_val_score(\n",
    "    model_more_iterations,\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "print(\"Previous Best Model (Less Regularization)\")\n",
    "print(\"Train average:     \", -less_regularization_train_scores.mean())\n",
    "print(\"Validation average:\", -less_regularization_val_scores.mean())\n",
    "print(\"Previous Model with This Solver\")\n",
    "print(\"Train average:     \", -alternative_solver_train_scores.mean())\n",
    "print(\"Validation average:\", -alternative_solver_val_scores.mean())\n",
    "print(\"Current Model\")\n",
    "print(\"Train average:     \", -more_iterations_train_scores.mean())\n",
    "print(\"Validation average:\", -more_iterations_val_scores.mean())"
=======
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# most_correlated should be a string\n",
    "assert type(most_correlated) == str\n",
    "\n",
    "# most_correlated should be one of the columns other than price\n",
    "assert most_correlated in diamonds.drop(\"price\", axis=1).columns"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "The results you got are most likely around 0.13241, whereas the previous model was around 0.13234. In other words, even after waiting all that time, we are getting 0.00007 worse log loss with this solver.\n",
    "\n",
    "This is a fairly typical experience when hyperparameter tuning! Often the default hyperparameters are the default because they work best in the most situations. This is especially true of logistic regression, which has relatively few hyperparameters. Once we get to more complex models, there are more \"levers to pull\" (hyperparameters to adjust) so it is more likely that we'll improve performance by deviating from the default.\n",
    "\n",
    "Let's declare the `model_less_regularization` to be our best model, and move on to the final evaluation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choose and Evaluate a Final Model"
=======
    "### Plotting the Predictor vs. Price\n",
    "\n",
    "We'll also create a scatter plot of that variable vs. `price`:"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "final_model = model_less_regularization"
=======
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Plot a sample of 1000 data points, most_correlated vs. price\n",
    "diamonds.sample(1000, random_state=1).plot.scatter(x=most_correlated, y=\"price\");"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "In order to evaluate our final model, we need to preprocess the full training and test data, fit the model on the full training data, and evaluate it on the full test data. Initially we'll continue to use log loss for the evaluation step.\n",
    "\n",
    "### Preprocessing the Full Dataset"
=======
    "### Setting Up Variables for Regression\n",
    "\n",
    "Declare `y` and `X_baseline` variables, where `y` is a Series containing `price` data and `X_baseline` is a DataFrame containing the column with the strongest correlation."
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
<<<<<<< HEAD
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit and transform X_train\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# Transform X_test\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate SMOTE with random_state=42 and sampling_strategy=0.28\n",
    "sm = SMOTE(random_state=42, sampling_strategy=0.28)\n",
    "# Fit and transform X_train_scaled and y_train using sm\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train_scaled, y_train)\n"
=======
    "y = diamonds['price']\n",
    "X_baseline = diamonds[[most_correlated]]"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Fitting the Model on the Full Training Data"
=======
    "The following code checks that you created valid `y` and `X_baseline` variables:"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight={1: 0.28}, l1_ratio=0.5,\n",
       "                   max_iter=2000, penalty='elasticnet', random_state=42,\n",
       "                   solver='saga')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "final_model.fit(X_train_oversampled, y_train_oversampled)"
=======
   "outputs": [],
   "source": [
    "# Run this code without changes\n",
    "\n",
    "# y should be a series\n",
    "assert type(y) == pd.Series\n",
    "\n",
    "# y should contain about 54k rows\n",
    "assert y.shape == (53940,)\n",
    "\n",
    "# X_baseline should be a DataFrame\n",
    "assert type(X_baseline) == pd.DataFrame\n",
    "\n",
    "# X_baseline should contain the same number of rows as y\n",
    "assert X_baseline.shape[0] == y.shape[0]\n",
    "\n",
    "# X_baseline should have 1 column\n",
    "assert X_baseline.shape[1] == 1"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Evaluating the Model on the Test Data\n",
    "\n",
    "#### Log Loss"
=======
    "### Creating and Fitting Simple Linear Regression\n",
    "\n",
    "The following code uses your variables to build and fit a simple linear regression."
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13023300952616806"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "log_loss(y_test, final_model.predict_proba(X_test_scaled))"
=======
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "import statsmodels.api as sm\n",
    "\n",
    "baseline_model = sm.OLS(y, sm.add_constant(X_baseline))\n",
    "baseline_results = baseline_model.fit()"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Great! We are getting slightly better performance when we train the model with the full training set, compared to the average cross-validated performance. This is typical since models tend to perform better with more training data.\n",
    "\n",
    "This model has improved log loss compared to our initial baseline model, which had about 0.172.\n",
    "\n",
    "But we're not quite done here!\n",
    "\n",
    "If we wanted to present this forest cover classification model to non-technical stakeholders, log loss would be a confusing choice. Let's compute some other metrics that tell the story of our model's performance in a more interpretable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "\n",
    "Although we noted the issues with accuracy as a metric on unbalanced datasets, accuracy is a very intuitive metric. Recall that we would expect an accuracy of about 0.928651 if we identified every cell as class 0. What accuracy do we get with our new model?\n",
    "\n",
    "(Note that we used `.predict_proba` above, but accuracy uses `.predict`)"
=======
    "## 3. Evaluate and Interpret Baseline Model Results\n",
    "\n",
    "Write any necessary code to evaluate the model performance overall and interpret its coefficients."
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "data": {
      "text/plain": [
       "0.9454602119260337"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, final_model.predict(X_test_scaled))"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  price   R-squared:                       0.849\n",
      "Model:                            OLS   Adj. R-squared:                  0.849\n",
      "Method:                 Least Squares   F-statistic:                 3.041e+05\n",
      "Date:                Mon, 02 Dec 2024   Prob (F-statistic):               0.00\n",
      "Time:                        01:02:26   Log-Likelihood:            -4.7273e+05\n",
      "No. Observations:               53940   AIC:                         9.455e+05\n",
      "Df Residuals:                   53938   BIC:                         9.455e+05\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const      -2256.3606     13.055   -172.830      0.000   -2281.949   -2230.772\n",
      "carat       7756.4256     14.067    551.408      0.000    7728.855    7783.996\n",
      "==============================================================================\n",
      "Omnibus:                    14025.341   Durbin-Watson:                   0.986\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           153030.525\n",
      "Skew:                           0.939   Prob(JB):                         0.00\n",
      "Kurtosis:                      11.035   Cond. No.                         3.65\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "print(baseline_results.summary())"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "In other words, our model correctly identifies the type of forest cover about 94.6% of the time, whereas always guessing the majority class (ponderosa pine) would only be accurate about 92.9% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "If we always chose the majority class, we would expect a precision of 0, since we would never identify any \"true positives\". What is the precision of our final model? Use `precision_score` from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html))."
=======
    "Then summarize your findings below:"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.6659836065573771"
=======
       "'Each additional Carat will then influence the price of the diamond by 7756'"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant function\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Display the precision score\n",
    "precision_score(y_test, final_model.predict(X_test_scaled))\n",
    "\n"
=======
    "# Your written answer here\n",
    "'Our Model is Statistically significant. Carat explains 85%  of the variance in price'\n",
    "'Each additional Carat will then influence the price of the diamond by 7756'"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "In other words, if our model labels a given cell of forest as class 1, there is about a 66.6% chance that it is actually class 1 (cottonwood/willow) and about a 33.4% chance that it is actually class 0 (ponderosa pine)."
=======
    "<details>\n",
    "    <summary style=\"cursor: pointer\"><b>Solution (click to expand)</b></summary>\n",
    "\n",
    "`carat` was the attribute most strongly correlated with `price`, therefore our model is describing this relationship.\n",
    "\n",
    "Overall this model is statistically significant and explains about 85% of the variance in price. In a typical prediction, the model is off by about &dollar;1k.\n",
    "\n",
    "* The intercept is at about -\\\\$2.3k. This means that a zero-carat diamond would sell for -\\\\$2.3k.\n",
    "* The coefficient for `carat` is about \\\\$7.8k. This means for each additional carat, the diamond costs about \\\\$7.8k more.\n",
    "\n",
    "</details>    "
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### Recall\n",
    "\n",
    "Again, if we always chose the majority class, we would expect a recall of 0. What is the recall of our final model? Use `recall_score` from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html))."
=======
    "## 4. Prepare a Categorical Feature for Multiple Regression Modeling\n",
    "\n",
    "Now let's go beyond our simple linear regression and add a categorical feature.\n",
    "\n",
    "### Identifying a Promising Predictor\n",
    "\n",
    "Below we create bar graphs for the categories present in each categorical feature:"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
<<<<<<< HEAD
      "text/plain": [
       "0.47307132459970885"
      ]
     },
     "execution_count": 34,
=======
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this code without changes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "categorical_features = diamonds.select_dtypes(\"object\").columns\n",
    "fig, axes = plt.subplots(ncols=len(categorical_features), figsize=(12,5))\n",
    "\n",
    "for index, feature in enumerate(categorical_features):\n",
    "    diamonds.groupby(feature).mean().plot.bar(\n",
    "        y=\"price\", ax=axes[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the name of the categorical predictor column you want to use in your model below. The choice here is more open-ended than choosing the numeric predictor above -- choose something that will be interpretable in a final model, and where the different categories seem to have an impact on the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "cat_col = 'cut'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code checks that you specified a column correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# cat_col should be a string\n",
    "assert type(cat_col) == str\n",
    "\n",
    "# cat_col should be one of the categorical columns\n",
    "assert cat_col in diamonds.select_dtypes(\"object\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Variables for Regression\n",
    "\n",
    "The code below creates a variable `X_iterated`: a DataFrame containing the column with the strongest correlation **and** your selected categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53936</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Ideal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53937</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.86</td>\n",
       "      <td>Premium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53940</th>\n",
       "      <td>0.75</td>\n",
       "      <td>Ideal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53940 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut\n",
       "1       0.23      Ideal\n",
       "2       0.21    Premium\n",
       "3       0.23       Good\n",
       "4       0.29    Premium\n",
       "5       0.31       Good\n",
       "...      ...        ...\n",
       "53936   0.72      Ideal\n",
       "53937   0.72       Good\n",
       "53938   0.70  Very Good\n",
       "53939   0.86    Premium\n",
       "53940   0.75      Ideal\n",
       "\n",
       "[53940 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "X_iterated = diamonds[[most_correlated, cat_col]]\n",
    "X_iterated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Categorical Variable\n",
    "\n",
    "If we tried to pass `X_iterated` as-is into `sm.OLS`, we would get an error. We need to use `pd.get_dummies` to create dummy variables for `cat_col`.\n",
    "\n",
    "**DO NOT** use `drop_first=True`, so that you can intentionally set a meaningful reference category instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut_Fair</th>\n",
       "      <th>cut_Good</th>\n",
       "      <th>cut_Ideal</th>\n",
       "      <th>cut_Premium</th>\n",
       "      <th>cut_Very Good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53936</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53937</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53940</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53940 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat  cut_Fair  cut_Good  cut_Ideal  cut_Premium  cut_Very Good\n",
       "1       0.23         0         0          1            0              0\n",
       "2       0.21         0         0          0            1              0\n",
       "3       0.23         0         1          0            0              0\n",
       "4       0.29         0         0          0            1              0\n",
       "5       0.31         0         1          0            0              0\n",
       "...      ...       ...       ...        ...          ...            ...\n",
       "53936   0.72         0         0          1            0              0\n",
       "53937   0.72         0         1          0            0              0\n",
       "53938   0.70         0         0          0            0              1\n",
       "53939   0.86         0         0          0            1              0\n",
       "53940   0.75         0         0          1            0              0\n",
       "\n",
       "[53940 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
<<<<<<< HEAD
    "# Import the relevant function\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Display the recall score\n",
    "recall_score(y_test, final_model.predict(X_test_scaled))"
=======
    "# Use pd.get_dummies to one-hot encode the categorical column in X_iterated\n",
    "X_iterated = pd.get_dummies(X_iterated, columns=[cat_col])\n",
    "X_iterated"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "In other words, if a given cell of forest is actually class 1, there is about a 47.9% chance that our model will correctly label it as class 1 (cottonwood/willow) and about a 52.1% chance that our model will incorrectly label it as class 0 (ponderosa pine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "Depending on the stakeholder, you most likely want to report just precision or just recall. Try to understand their business case:\n",
    "\n",
    "* If false positives are a bigger problem (labeled cottonwood/willow when it's really ponderosa pine), precision is the important metric to report\n",
    "* If false negatives are a bigger problem (labeled ponderosa pine when it's really cottonwood/willow), recall is the important metric to report\n",
    "\n",
    "If those problems have truly equal importance, you could report an f1-score instead, although this is somewhat more difficult for the average person to interpret.\n",
    "\n",
    "#### BONUS: Adjusting the Decision Threshold\n",
    "\n",
    "If either of those problems is important enough that it outweighs overall accuracy, you could also adjust the decision threshold of your final model to improve the metric that matters most. Let's say that it's important to improve the recall score — that we want to be able to correctly label more of the cottonwood/willow trees as cottonwood/willow trees, even if that means accidentally labeling more ponderosa pine as cottonwood/willow incorrectly.\n",
    "\n",
    "Then we can use `.predict_proba` to err on the side of the positive class. Let's use an exaggerated example, which assumes that false negatives are a very significant problem. Instead of using the default 50% threshold (where a probability over 0.5 is classified as positive) let's say that if there is greater than a 1% chance it's positive, we classify it as positive:\n",
    "\n",
    "(If the opposite issue were the case — it's very important that every area classified as 1 is actually cottonwood/willow — you would want the threshold to be higher than 50% rather than lower than 50%.)"
=======
    "The following code checks that you have the right number of columns:"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# X_iterated should be a dataframe\n",
    "assert type(X_iterated) == pd.DataFrame\n",
    "\n",
    "# You should have the number of unique values in one of the\n",
    "# categorical columns + 1 (representing the numeric predictor)\n",
    "valid_col_nums = diamonds.select_dtypes(\"object\").nunique() + 1\n",
    "\n",
    "# Check that there are the correct number of columns\n",
    "# (if this crashes, make sure you did not use `drop_first=True`)\n",
    "assert X_iterated.shape[1] in valid_col_nums.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, applying your domain understanding, **choose a column to drop and drop it**. This category should make sense as a \"baseline\" or \"reference\". For the \"cut_Very Good\" column that was generated when `pd.get_dummies` was used, we need to remove the space in the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "X_iterated =  X_iterated.drop('cut_Very Good',axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to change the boolean values for the four \"cut\" column to 1s and 0s in order for the regression to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   "metadata": {},
   "outputs": [
    {
     "data": {
<<<<<<< HEAD
      "text/plain": [
       "0    0.581446\n",
       "1    0.418554\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
=======
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut_Fair</th>\n",
       "      <th>cut_Good</th>\n",
       "      <th>cut_Ideal</th>\n",
       "      <th>cut_Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53936</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53937</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53940</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53940 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat  cut_Fair  cut_Good  cut_Ideal  cut_Premium\n",
       "1       0.23         0         0          1            0\n",
       "2       0.21         0         0          0            1\n",
       "3       0.23         0         1          0            0\n",
       "4       0.29         0         0          0            1\n",
       "5       0.31         0         1          0            0\n",
       "...      ...       ...       ...        ...          ...\n",
       "53936   0.72         0         0          1            0\n",
       "53937   0.72         0         1          0            0\n",
       "53938   0.70         0         0          0            0\n",
       "53939   0.86         0         0          0            1\n",
       "53940   0.75         0         0          1            0\n",
       "\n",
       "[53940 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Run this cell without changes\n",
    "\n",
    "def final_model_func(model, X):\n",
    "    \"\"\"\n",
    "    Custom function to predict probability of\n",
    "    cottonwood/willow. If the model says there\n",
    "    is >1% chance, we label it as class 1\n",
    "    \"\"\"\n",
    "    probs = model.predict_proba(X)[:,1]\n",
    "    return [int(prob > 0.01) for prob in probs]\n",
    "\n",
    "# Calculate predictions with adjusted threshold and\n",
    "# display proportions of predictions\n",
    "threshold_adjusted_probs = pd.Series(final_model_func(final_model, X_test_scaled))\n",
    "threshold_adjusted_probs.value_counts(normalize=True)"
=======
    "# Your code here\n",
    "cut_columns = [col for col in X_iterated.columns if col.startswith('cut_')]\n",
    "X_iterated[cut_columns] = X_iterated[cut_columns].astype(int)\n",
    "X_iterated"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "So, now we are predicting that everything over a 1% chance of being class 1 as class 1, which means that we're classifying about 58.6% of the records as class 0 and 41.4% of the records as class 1."
=======
    "Now you should have 1 fewer column than before:"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Check that there are the correct number of columns\n",
    "assert X_iterated.shape[1] in (valid_col_nums - 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build a Multiple Linear Regression Model\n",
    "\n",
    "Using the `y` variable from our previous model and `X_iterated`, build a model called `iterated_model` and a regression results object called `iterated_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant to X_iterated for the intercept term\n",
    "X_iterated_with_const = sm.add_constant(X_iterated)\n",
    "\n",
    "# Fit the OLS regression model\n",
    "iterated_model = sm.OLS(y, X_iterated_with_const)\n",
    "\n",
    "# Get the regression results\n",
    "iterated_results = iterated_model.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate and Interpret Multiple Linear Regression Model Results\n",
    "\n",
    "If the model was set up correctly, the following code will print the results summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Accuracy: 0.6515686681903179\n",
      "Recall:   0.9912663755458515\n"
=======
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  price   R-squared:                       0.856\n",
      "Model:                            OLS   Adj. R-squared:                  0.856\n",
      "Method:                 Least Squares   F-statistic:                 6.437e+04\n",
      "Date:                Mon, 02 Dec 2024   Prob (F-statistic):               0.00\n",
      "Time:                        01:02:28   Log-Likelihood:            -4.7142e+05\n",
      "No. Observations:               53940   AIC:                         9.429e+05\n",
      "Df Residuals:                   53934   BIC:                         9.429e+05\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const       -2365.3343     17.781   -133.026      0.000   -2400.185   -2330.484\n",
      "carat        7871.0821     13.980    563.040      0.000    7843.682    7898.482\n",
      "cut_Fair    -1510.1354     40.240    -37.528      0.000   -1589.006   -1431.265\n",
      "cut_Good     -389.8036     25.595    -15.230      0.000    -439.970    -339.637\n",
      "cut_Ideal     290.7886     17.239     16.868      0.000     257.000     324.577\n",
      "cut_Premium   -71.0583     18.872     -3.765      0.000    -108.048     -34.068\n",
      "==============================================================================\n",
      "Omnibus:                    14616.138   Durbin-Watson:                   1.027\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           150962.278\n",
      "Skew:                           1.007   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.944   Cond. No.                         9.08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
<<<<<<< HEAD
    "print(\"Accuracy:\", accuracy_score(y_test, threshold_adjusted_probs))\n",
    "print(\"Recall:  \", recall_score(y_test, threshold_adjusted_probs))"
=======
    "print(iterated_results.summary())"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "This means that we are able to identify 99.1% of the true positives (i.e. 99.1% of the cottonwood/willow cells are identified). However this comes at a cost; our overall accuracy is now 65.7% instead of over 90%.\n",
    "\n",
    "So we are classifying over 40% of the cells as cottonwood/willow, even though fewer than 10% of the cells are actually that category, in order to miss as few true positives as possible. Even though this seems fairly extreme, our model is still better than just choosing class 1 every time (that model would have about 7% accuracy).\n",
    "\n",
    "This kind of model might be useful if there is some kind of treatment needed for cottonwood/willow trees, but your organization only has the resources to visit fewer than half of the study areas. This model would allow them to visit 41% of the areas and successfully treat over 99% of the cottonwood/willow trees.\n",
    "\n",
    "You can also imagine a less-drastic version of this threshold adjusting, where you trade off a marginal improvement in precision or recall for a marginal reduction in accuracy. Visually inspecting the precision-recall curve ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_precision_recall_curve.html)) can help you choose the threshold based on what you want to optimize."
=======
    "Summarize your findings below. How did the iterated model perform overall? How does this compare to the baseline model? What do the coefficients mean?\n",
    "\n",
    "Create as many additional cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Ideal cut adds value to a diamond compared to Very Good, while Fair and Good cuts reduce the price substantially.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your written answer here\n",
    "'Carat remains the most influential predictor of price, but cut introduces meaningful variation.'\n",
    "'The Ideal cut adds value to a diamond compared to Very Good, while Fair and Good cuts reduce the price substantially.'"
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
<<<<<<< HEAD
    "In this lab, you completed an end-to-end machine learning modeling process with logistic regression on an imbalanced dataset. First you built and evaluated a baseline model. Next you wrote a custom cross validation function in order to use SMOTE resampling appropriately (without needing an `imblearn` pipeline). After that, you tuned the model through adjusting the regularization strength and the gradient descent hyperparameters. Finally, you evaluated your final model on log loss as well as more user-friendly metrics."
=======
    "Congratulations, you completed an iterative linear regression process! You practiced developing a baseline and an iterated model, as well as identifying promising predictors from both numeric and categorical features."
>>>>>>> a6edc1941bc8bc98dbcfc9805689eb0e19012d84
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
